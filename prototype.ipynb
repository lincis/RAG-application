{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infisical import InfisicalClient\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from openai import OpenAI\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import Session, DeclarativeBase, mapped_column\n",
    "from pgvector.sqlalchemy import Vector\n",
    "\n",
    "import json\n",
    "\n",
    "if_client = InfisicalClient(token = os.environ.get('INFISICAL_TOKEN'))\n",
    "\n",
    "DWH_USER   = if_client.get_secret('DWH_PG_USER')\n",
    "DWH_PW     = if_client.get_secret('DWH_PG_PW')\n",
    "DWH_HOST   = if_client.get_secret('DWH_PG_HOST')\n",
    "DWH_DBNAME = if_client.get_secret('DWH_PG_DBNAME')\n",
    "\n",
    "postgres_conn = f'postgresql+psycopg2://{DWH_USER.secret_value}:{DWH_PW.secret_value}@{DWH_HOST.secret_value}:5432/{DWH_DBNAME.secret_value}'\n",
    "\n",
    "model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = if_client.get_secret('OPENAI_API_KEY').secret_value\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, model = 'gpt-3.5-turbo-1106', client = openai_client):\n",
    "    return client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = prompt,\n",
    "        stream = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "class SparkEmbeddings(Base):\n",
    "    __tablename__ = \"spark_embeddings\"\n",
    "    spark_id = mapped_column(sa.VARCHAR(36), primary_key = True)\n",
    "    map_id = mapped_column(sa.VARCHAR(36))\n",
    "    entity_updated = mapped_column(sa.DateTime)\n",
    "    title = mapped_column(sa.String)\n",
    "    fulltext = mapped_column(sa.String)\n",
    "    embedding = mapped_column(Vector(384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-22 15:11:49,396 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2023-11-22 15:11:49,397 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2023-11-22 15:11:49,505 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2023-11-22 15:11:49,507 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2023-11-22 15:11:49,613 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2023-11-22 15:11:49,614 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2023-11-22 15:11:49,728 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2023-11-22 15:11:49,738 INFO sqlalchemy.engine.Engine SELECT spark_embeddings.spark_id AS spark_embeddings_spark_id, spark_embeddings.map_id AS spark_embeddings_map_id, spark_embeddings.entity_updated AS spark_embeddings_entity_updated, spark_embeddings.title AS spark_embeddings_title, spark_embeddings.fulltext AS spark_embeddings_fulltext, spark_embeddings.embedding AS spark_embeddings_embedding \n",
      "FROM spark_embeddings ORDER BY spark_embeddings.embedding <-> %(embedding_1)s \n",
      " LIMIT %(param_1)s\n",
      "2023-11-22 15:11:49,740 INFO sqlalchemy.engine.Engine [generated in 0.00557s] {'embedding_1': '[-0.03260240703821182,-0.03894404321908951,-0.0876707136631012,-0.028885889798402786,0.05022039636969566,0.027655594050884247,0.05529113486409187,0.0 ... (7775 characters truncated) ... 72,0.02622086927294731,0.008653806522488594,0.025670567527413368,-0.03317273035645485,0.018552377820014954,-0.007830153219401836,0.06484249234199524]', 'param_1': 5}\n",
      "2023-11-22 15:11:50,027 INFO sqlalchemy.engine.Engine ROLLBACK\n",
      "Weather is the great equalizer\n",
      "Methods for mass deliberation\n",
      "Are seasonal mood changes rational?\n",
      "How Weather and Mood are connected.\n",
      "Our environment is powerful\n"
     ]
    }
   ],
   "source": [
    "query = 'Why is the sky blue?'\n",
    "q_embedding = model.encode(query)\n",
    "\n",
    "engine = create_engine(postgres_conn, echo = True, echo_pool = True)\n",
    "\n",
    "# create session and add objects\n",
    "with Session(engine) as session:\n",
    "    query_results = session.query(SparkEmbeddings).\\\n",
    "        order_by(SparkEmbeddings.embedding.l2_distance(q_embedding)).\\\n",
    "        limit(5).all()\n",
    "\n",
    "# print(type(query_results))\n",
    "for r in query_results:\n",
    "    print(r.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdocs = json.dumps([{\n",
    "    'source': f'<spark_id=\"r.spark_id\"/>',\n",
    "    'title': r.title,\n",
    "    'content': r.fulltext,\n",
    "} for r in query_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You're an honest, helpful and polite assistent answering questions from the provided context and only from context. If the context is not enough, you honestly admit that there is not enough data to answer the question.\"},\n",
    "    {\"role\": \"system\", \"content\": \"The context is provided as JSON array of objects with title, content and source keys, where content is the text and source is the source of the text. For each and every generated sentence you must provide the source of the sentence.\"},\n",
    "    {\"role\": \"system\", \"content\": \"Context: \" + qdocs},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm afraid the provided context does not contain relevant information to answer that question."
     ]
    }
   ],
   "source": [
    "stream = llm(prompt)\n",
    "for part in stream:\n",
    "    print(part.choices[0].delta.content or \"\", end = \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
